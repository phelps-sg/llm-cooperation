# Abstract

As large language models (LLMs) continue to advance, understanding the emergent
goal-like behaviors and cooperative tendencies of artificially generated agents
becomes crucial for AI alignment research. This paper outlines an
interdisciplinary research agenda that combines insights from cognitive
science, artificial intelligence, and experimental economics to explore the
cooperative and competitive dynamics of simulacra instantiated by LLM prompts.

We propose a series of experimental economics simulations, including the
Prisoner's Dilemma, public goods games, and other well-established frameworks,
to evaluate the propensity of LLM-generated simulacra to cooperate under
various conditions. These experiments will enable us to assess the goal-like
behaviors that emerge, and whether these behaviors align with human values and
cooperation norms.

In addition to detailing the experimental design, we will discuss the potential
implications of our findings for the development of AI alignment strategies and
safe AGI. By elucidating the factors that govern cooperative behavior in
LLM-generated simulacra, this research aims to contribute to the broader
understanding of the emergent properties of AI systems and inform the design of
models that better align with human values and societal goals.

```{tableofcontents}
```
